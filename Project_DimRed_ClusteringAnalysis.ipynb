{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction and Clustering Analysis\n",
    "### By: Swati Kohli, Poonam Patil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "**The project is to predict the house sale price. There are two methods explored to perform prediction.**\n",
    "\n",
    "**Part 1- Dimension reduction:**  \n",
    "The method used in this part involves using different dimension reduction techniques for numerical and categorical featues and building linear regression model with ridge regularization. The model results are then compared with base model to test the performance.\n",
    "\n",
    "**Part 2: Clustering Analysis**  \n",
    "The method in this part clusterings analysis with numerical and categorical data combined. Gower distance metric is  used to measure the distance. The Gower distance is essentially a special distance metric that measures numerical data and categorical data separately, then combine them to form a distance calculation.\n",
    "\n",
    "Data for the project is taken from below website:\n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prince\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import itertools\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "data = pd.read_csv('train.csv')\n",
    "data = data.drop('Id', axis = 1)\n",
    "\n",
    "# Remove columns that have too many missing values\n",
    "data = data.drop(data.columns[data.isnull().sum() > 30], axis = 1)\n",
    "\n",
    "# Remove missing values\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\n",
       "0          60       RL     8450   Pave      Reg         Lvl    AllPub   \n",
       "1          20       RL     9600   Pave      Reg         Lvl    AllPub   \n",
       "2          60       RL    11250   Pave      IR1         Lvl    AllPub   \n",
       "3          70       RL     9550   Pave      IR1         Lvl    AllPub   \n",
       "4          60       RL    14260   Pave      IR1         Lvl    AllPub   \n",
       "\n",
       "  LotConfig LandSlope Neighborhood  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0    Inside       Gtl      CollgCr  ...             0         0           0   \n",
       "1       FR2       Gtl      Veenker  ...             0         0           0   \n",
       "2    Inside       Gtl      CollgCr  ...             0         0           0   \n",
       "3    Corner       Gtl      Crawfor  ...           272         0           0   \n",
       "4       FR2       Gtl      NoRidge  ...             0         0           0   \n",
       "\n",
       "  PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition SalePrice  \n",
       "0        0        0       2    2008        WD        Normal    208500  \n",
       "1        0        0       5    2007        WD        Normal    181500  \n",
       "2        0        0       9    2008        WD        Normal    223500  \n",
       "3        0        0       2    2006        WD       Abnorml    140000  \n",
       "4        0        0      12    2008        WD        Normal    250000  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "del X['SalePrice']\n",
    "y = data['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train_valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y , test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further split train_valid dataset into train and valid set for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.2 , random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing the train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate numerical and categorical features into different datasets\n",
    "X_train_num = X_train.select_dtypes(include=[np.number])\n",
    "X_train_cat = X_train.select_dtypes(exclude=[np.number])\n",
    "\n",
    "X_valid_num = X_valid.select_dtypes(include=[np.number])\n",
    "X_valid_cat = X_valid.select_dtypes(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure the training feature and testing feature has same number of levels\n",
    "keep = X_train_cat.nunique() == X_valid_cat.nunique()\n",
    "X_train_cat = X_train_cat[X_train_cat.columns[keep]]\n",
    "X_valid_cat =X_valid_cat[X_valid_cat.columns[keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical features that have same levels, make sure the classes are the same\n",
    "keep = []\n",
    "for i in range(X_train_cat.shape[1]):\n",
    "    keep.append(all(np.sort(X_train_cat.iloc[:,i].unique()) == np.sort(X_valid_cat.iloc[:,i].unique())))\n",
    "X_train_cat = X_train_cat[X_train_cat.columns[keep]]\n",
    "X_valid_cat = X_valid_cat[X_valid_cat.columns[keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create hyperparameter set\n",
    "alphas = np.logspace(-2,2,5)            # parameter in ridge regression\n",
    "n_pca_components =[25,30,34]            # for n_components parameter in KernelPCA\n",
    "kernels= ['rbf', 'sigmoid']             # wrapper for rbf and sigmoid KernelPCA model\n",
    "gammas = np.linspace(0.03, 0.05, 3)     # parameter in KernelPCA\n",
    "n_mca_components = [5,7,9,11]           # for n_components parameter in MCA\n",
    "n_iters = np.logspace(1,2,3, dtype=int) # for n_iter parameter in MCA\n",
    "\n",
    "hyperparam_set = list(itertools.product(alphas, n_pca_components, kernels, gammas, n_mca_components, n_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numerical data before applying PCA       \n",
    "scaler= StandardScaler() \n",
    "scaler.fit(X_train_num)      # fit scaler on train set\n",
    "X_train_num = pd.DataFrame(scaler.transform(X_train_num)) # transform train set\n",
    "X_valid_num = pd.DataFrame(scaler.transform(X_valid_num)) # transform validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a wrapper to select the best model and best hyperparameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameter set: (0.1, 34, 'sigmoid', 0.03, 11, 10)\n",
      "MSE on validation set: 985171890.5521743\n"
     ]
    }
   ],
   "source": [
    "valid_score =[]     # initialize a list to store validation score\n",
    "\n",
    "for params in hyperparam_set:\n",
    "    \n",
    "    # train KernelPCA on numerical data\n",
    "    pca = KernelPCA(n_components = params[1], kernel=params[2], gamma=params[3], fit_inverse_transform = True) \n",
    "    X_train_pca = pd.DataFrame(pca.fit_transform(X_train_num))\n",
    "    X_valid_pca = pd.DataFrame(pca.transform(X_valid_num))\n",
    "    \n",
    "    # train MCA on categorical data\n",
    "    mca = prince.MCA(n_components=params[4], n_iter=params[5], copy=True, check_input=True, benzecri=False, \n",
    "                 random_state= 1, engine='auto')\n",
    "    mca=mca.fit(X_train_cat)        # fit MCA on X_train_cat data\n",
    "    \n",
    "    X_train_mca = mca.transform(X_train_cat)     # tranform X_train_cat data on MCA\n",
    "    X_valid_mca = mca.transform(X_valid_cat)     # tranform X_valid_cat data on MCA\n",
    "    \n",
    "    # StandardScaler reset the row indexes. \n",
    "    # Therefore we need to reset the index for MCA components too. As we need to combine these two datasets (PCA+MCA)\n",
    "    X_train_mca = X_train_mca.reset_index(drop=True)\n",
    "    X_valid_mca = X_valid_mca.reset_index(drop=True)\n",
    "    \n",
    "    # combine PCA and MCA to form complete training dataset\n",
    "    X_train = pd.concat([X_train_pca, X_train_mca], axis=1, ignore_index=True)\n",
    "    X_valid = pd.concat([X_valid_pca, X_valid_mca], axis=1, ignore_index=True)\n",
    "    \n",
    "    # fit linear regression with Ridge regularization on dimensally reduced components (i.e. PCA+MCA)\n",
    "    lm= linear_model.Ridge(alpha=params[0])\n",
    "    lm.fit(X_train, y_train)\n",
    "    valid_score.append(metrics.mean_squared_error(lm.predict(X_valid), y_valid))\n",
    "\n",
    "print('best hyperparameter set:', hyperparam_set[np.argmin(valid_score)])\n",
    "print('MSE on validation set:', min(valid_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation: The best model selected is sigmoid kernel basis function for dimension reduction. All PCA and MCA components are selected in parameter tuning. Best gamma is 0.03 and best n_iters =10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparam = hyperparam_set[np.argmin(valid_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing best hypeerparameters into variables to be used in training the final model\n",
    "best_alpha = best_hyperparam[0]\n",
    "best_n_pca_components = best_hyperparam[1]\n",
    "best_kernel = best_hyperparam[2]\n",
    "best_gamma = best_hyperparam[3]\n",
    "best_n_mca_components = best_hyperparam[4]\n",
    "best_n_iters = best_hyperparam[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training the model on train_valid set with best model and hyperparameters selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing train_valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate numerical and categorical features into different datasets\n",
    "X_train_valid_num = X_train_valid.select_dtypes(include=[np.number])\n",
    "X_train_valid_cat = X_train_valid.select_dtypes(exclude=[np.number])\n",
    "\n",
    "X_test_num = X_test.select_dtypes(include=[np.number])\n",
    "X_test_cat = X_test.select_dtypes(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure the training feature and testing feature has same number of levels\n",
    "keep = X_train_valid_cat.nunique() == X_test_cat.nunique()\n",
    "X_train_valid_cat = X_train_valid_cat[X_train_valid_cat.columns[keep]]\n",
    "X_test_cat =X_test_cat[X_test_cat.columns[keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical features that have same levels, make sure the classes are the same\n",
    "keep = []\n",
    "for i in range(X_train_valid_cat.shape[1]):\n",
    "    keep.append(all(np.sort(X_train_valid_cat.iloc[:,i].unique()) == np.sort(X_test_cat.iloc[:,i].unique())))\n",
    "X_train_valid_cat = X_train_valid_cat[X_train_valid_cat.columns[keep]]\n",
    "X_test_cat = X_test_cat[X_test_cat.columns[keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numerical data before applying PCA        \n",
    "scaler= StandardScaler() \n",
    "scaler.fit(X_train_valid_num)      # fit scaler on train set\n",
    "X_train_valid_num = pd.DataFrame(scaler.transform(X_train_valid_num)) # transform train set\n",
    "X_test_num = pd.DataFrame(scaler.transform(X_test_num)) # transform validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Employing KernelPCA and MCA dimention reduction techniques on train_valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train PCA on numerical data\n",
    "pca = KernelPCA(n_components = best_n_pca_components, kernel=best_kernel, gamma=best_gamma, fit_inverse_transform = True) \n",
    "X_train_valid_pca = pd.DataFrame(pca.fit_transform(X_train_valid_num))\n",
    "X_test_pca = pd.DataFrame(pca.transform(X_test_num))\n",
    "\n",
    "# train MCA on categorical data\n",
    "mca = prince.MCA(n_components=best_n_mca_components, n_iter= best_n_iters, copy=True, check_input=True, benzecri=False, \n",
    "             random_state= 1, engine='auto')\n",
    "mca=mca.fit(X_train_valid_cat)\n",
    "\n",
    "X_train_valid_mca = mca.transform(X_train_valid_cat)\n",
    "X_test_mca  = mca.transform(X_test_cat)\n",
    "\n",
    "## StandardScaler reset the row indexes for numerical data. Therefore reset the index MCA components too\n",
    "# reset index of MCA components\n",
    "X_train_valid_mca = X_train_valid_mca.reset_index(drop=True)\n",
    "X_test_mca = X_test_mca.reset_index(drop=True)\n",
    "\n",
    "# combine PCA and MCA to form complete training dataset\n",
    "X_train_valid = pd.concat([X_train_valid_pca, X_train_valid_mca], axis=1, ignore_index=True)\n",
    "X_test = pd.concat([X_test_pca, X_test_mca], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fitting linear regression with ridge regularization on train_valid dataset with best hyperparameters selcted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 947741457.1330783\n"
     ]
    }
   ],
   "source": [
    "# fit linear regression with Ridge regularization on dimensally reduced components (i.e. PCA+MCA)\n",
    "lm= linear_model.Ridge(alpha=best_alpha)\n",
    "lm.fit(X_train_valid, y_train_valid)\n",
    "test_score = metrics.mean_squared_error(lm.predict(X_test), y_test)\n",
    "print('Test MSE:', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Testing results with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for the categorical variables in train_valid dataset\n",
    "X_train_valid_dummy = pd.get_dummies(X_train_valid_cat, drop_first=True)\n",
    "X_train_valid_dummy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for the categorical variables in test dataset\n",
    "X_test_dummy = pd.get_dummies(X_test_cat, drop_first = True)\n",
    "X_test_dummy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate numerical and dummy features to create a data for modelling\n",
    "X_train_valid = pd.concat([X_train_valid_num, X_train_valid_dummy], axis=1, ignore_index=True)\n",
    "X_test = pd.concat([X_test_num, X_test_dummy], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE with base model: 1003333118.03285\n"
     ]
    }
   ],
   "source": [
    "# Train linear regression model with Ridge regularization\n",
    "lm_reg= linear_model.Ridge(alpha=best_alpha)\n",
    "lm_reg.fit(X_train_valid, y_train_valid)   # fit model on train_valid set\n",
    "test_score = metrics.mean_squared_error(lm_reg.predict(X_test), y_test)  # test model on test set\n",
    "print('Test MSE with base model:', test_score)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Comparison of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we can see that, in the linear regression with dimension reducted components, best MSE on test set is achieved when the model used all of the PCA and MCA components. However, the results with a linear model using sigmoid kernel basis function for dimension reducition gives best prediction of house sale price than the simple linear regression with ridge regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering analysis can be performed with numerical and categorical data combined. Distance metric is used for the same. Further, Gower distance is used which is a special distance metric that measures numerical data and categorical data separately,then combine them to form a distance calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**  \n",
    "Seperate predictor and response variables (X and y) from the dataset.\n",
    "1. Install and import Gower Library for mixed data types.\n",
    "2. Compute the Gower distance of the full predictors set\n",
    "3. Apply K-medoids using the gower distance matrix as input to get clusters and medoids(index).\n",
    "4. Compare the clustering result with the 'ground truth'.  \n",
    "    i. Get array of cluster membership of each observation.  \n",
    "    ii. Ground Truth = Bin the response variable (of the original data set) into the number of categories as per k clusters.  \n",
    "    iii. For Clustering performance, compute the normalized mutual information (NMI) between clustering results and the binned categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. \n",
    "# pip install gower\n",
    "import gower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.24024484, 0.03944052, ..., 0.19145243, 0.25093865,\n",
       "        0.20125039],\n",
       "       [0.24024484, 0.        , 0.24959742, ..., 0.24499281, 0.19383048,\n",
       "        0.1789145 ],\n",
       "       [0.03944052, 0.24959742, 0.        , ..., 0.20383762, 0.2743255 ,\n",
       "        0.22117272],\n",
       "       ...,\n",
       "       [0.19145243, 0.24499281, 0.20383762, ..., 0.        , 0.25380814,\n",
       "        0.25413764],\n",
       "       [0.25093865, 0.19383048, 0.2743255 , ..., 0.25380814, 0.        ,\n",
       "        0.16829133],\n",
       "       [0.20125039, 0.1789145 , 0.22117272, ..., 0.25413764, 0.16829133,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2.\n",
    "# Find the Gower distance matrix of the full predictors set\n",
    "gd_matrix = gower.gower_matrix(X)\n",
    "gd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "#### Let's take k = 5 clusters. Hierachical clustering can also be done for deciding k as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial centers:  [ 109 1313  132   70  229]\n",
      "New Medoids are: [712, 210, 1242, 793, 729]\n",
      "/nCluster output example [712, 7, 10, 34, 37, 45, 55, 78, 81, 91, 97, 101, 102, 109, 113, 116, 124, 135, 144, 152, 166, 171, 187, 194, 196, 205, 209, 215, 225, 227, 232, 234, 244, 252, 264, 268, 272, 293, 297, 298, 312, 318, 329, 338, 355, 362, 363, 367, 386, 415, 421, 424, 428, 429, 444, 454, 463, 470, 474, 482, 497, 499, 504, 514, 515, 528, 558, 568, 580, 585, 592, 603, 609, 616, 619, 623, 630, 645, 652, 659, 664, 670, 676, 692, 698, 706, 710, 714, 725, 730, 738, 740, 754, 763, 767, 772, 774, 777, 782, 788, 792, 807, 815, 822, 827, 834, 839, 842, 850, 854, 855, 856, 859, 881, 886, 888, 889, 892, 939, 940, 941, 945, 950, 957, 965, 986, 988, 991, 997, 999, 1011, 1018, 1035, 1038, 1039, 1042, 1047, 1061, 1067, 1077, 1093, 1098, 1100, 1105, 1153, 1155, 1162, 1180, 1187, 1199, 1215, 1217, 1223, 1236, 1240, 1243, 1264, 1265, 1269, 1270, 1273, 1274, 1278, 1282, 1285, 1287, 1301, 1302, 1307, 1328, 1338, 1348, 1370, 1411, 1415, 1416, 1430, 1437]\n"
     ]
    }
   ],
   "source": [
    "# Step 3.\n",
    "# pip install pyclustering\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "\n",
    "# To use the K-medoid function, provide some initial centers. Let's take k =5. \n",
    "\n",
    "# Randomly sample 5 observations as centers.\n",
    "np.random.seed(50)\n",
    "k = 5\n",
    "center_index = np.random.randint(0, len(y), k)\n",
    "print(\"Initial centers: \",center_index)\n",
    "\n",
    "# Create K-Medoids algorithm for processing distance matrix instead of points\n",
    "kmedoids_instance = kmedoids(gd_matrix, center_index, data_type='distance_matrix')\n",
    "\n",
    "# Run cluster analysis and obtain results\n",
    "kmedoids_instance.process()\n",
    "\n",
    "clusters = kmedoids_instance.get_clusters() \n",
    "medoids = kmedoids_instance.get_medoids() # new medoids\n",
    "print(\"New Medoids are:\",medoids)\n",
    "\n",
    "# Cluster output is given as nested list of clusters(k)\n",
    "print(\"/nCluster output example\",clusters[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation: Labels are not produced. Instead, clusters are made as a list which has index of observations that belong to the respective cluster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 4, ..., 4, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4.\n",
    "\n",
    "# Step 4.i.\n",
    "# Assign labels to Clusters\n",
    "cluster_labels = np.zeros([X.shape[0]], dtype=int)\n",
    "for i in range(1,k):\n",
    "    cluster_labels[clusters[i]] = i\n",
    "\n",
    "cluster_labels #array that records the cluster membership of each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    3\n",
       "2    3\n",
       "3    1\n",
       "4    4\n",
       "5    1\n",
       "6    4\n",
       "7    3\n",
       "8    1\n",
       "9    0\n",
       "Name: SalePrice, dtype: category\n",
       "Categories (5, int64): [0 < 1 < 2 < 3 < 4]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4.ii.\n",
    "# Bin response variable so each group has roughly the same number of observations.\n",
    "response_binned = pd.qcut(y, q = k, labels = range(k))\n",
    "response_binned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins are made on the basis of increasing selling price range\n",
    "(34899.999, 124000.0] < (124000.0, 147000.0] < (147000.0, 179000.0] < (179000.0, 230000.0] < (230000.0, 755000.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check performance with NMI performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI score with clusters/K = 5: 0.24936514664331721\n"
     ]
    }
   ],
   "source": [
    "# Step4.iii.\n",
    "# NMI between clustering results and the binned categories.\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "NMI_clusters5 = normalized_mutual_info_score(response_binned, cluster_labels)\n",
    "print(\"NMI score with clusters/K = 5:\", NMI_clusters5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "Given the 'ground truth' class assignments of the samples, NMI is used to evaluate the performance of the clustering technique by measuring the similarity between the results of clustering and 'ground truth'. In this case, ground truth is taken as binned response variable(using pandas.qcut function so each group has roughly the same number of observations).  \n",
    "\n",
    "Low NMI indicates classes members are predominantly split across different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Since, NMI is normalized, we can use it to compare clusterings with different numbers of clusters. \n",
    "### Let's compare different K values(clusters) to see the change in results.\n",
    "#### K values used for the analysis are 4,3 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial centers:  [ 109 1313  132   70]\n",
      "New Medoids are: [216, 1242, 1073, 168]\n",
      "NMI score with clusters/K = 4: 0.2571355084670131\n"
     ]
    }
   ],
   "source": [
    "# Step 3.\n",
    "\n",
    "# Randomly sample 4 observations as initial centers.\n",
    "np.random.seed(50)\n",
    "k = 4\n",
    "center_index = np.random.randint(0, len(y), k)\n",
    "print(\"Initial centers: \",center_index)\n",
    "\n",
    "# create K-Medoids algorithm for processing distance matrix instead of points\n",
    "kmedoids_instance = kmedoids(gd_matrix, center_index, data_type='distance_matrix')\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedoids_instance.process()\n",
    "\n",
    "clusters = kmedoids_instance.get_clusters() \n",
    "medoids = kmedoids_instance.get_medoids() # new medoids\n",
    "print(\"New Medoids are:\",medoids)\n",
    "\n",
    "\n",
    "# Step 4.\n",
    "\n",
    "# Step 4.i.\n",
    "# cluster labels\n",
    "cluster_labels = np.zeros([X.shape[0]], dtype=int)\n",
    "for i in range(1,k):\n",
    "    cluster_labels[clusters[i]] = i\n",
    "\n",
    "# Step 4.ii.\n",
    "# Bin response variable to roughly so each group has roughly the same number of observations.\n",
    "response_binned = pd.qcut(y, q = k, labels = range(k))\n",
    "\n",
    "# Step4.iii.\n",
    "# NMI between clustering results and the binned categories.\n",
    "NMI_clusters4 = normalized_mutual_info_score(response_binned, cluster_labels)\n",
    "print(\"NMI score with clusters/K = 4:\", NMI_clusters4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial centers:  [ 109 1313  132]\n",
      "New Medoids are: [602, 1242, 1446]\n",
      "NMI score with clusters/K = 3: 0.27257573457056755\n"
     ]
    }
   ],
   "source": [
    "# Step 3.\n",
    "\n",
    "# Randomly sample 4 observations as initial centers.\n",
    "np.random.seed(50)\n",
    "k = 3\n",
    "center_index = np.random.randint(0, len(y), k)\n",
    "print(\"Initial centers: \",center_index)\n",
    "\n",
    "# create K-Medoids algorithm for processing distance matrix instead of points\n",
    "kmedoids_instance = kmedoids(gd_matrix, center_index, data_type='distance_matrix')\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedoids_instance.process()\n",
    "\n",
    "clusters = kmedoids_instance.get_clusters() \n",
    "medoids = kmedoids_instance.get_medoids() # new medoids\n",
    "print(\"New Medoids are:\",medoids)\n",
    "\n",
    "\n",
    "# Step 4.\n",
    "\n",
    "# Step 4.i.\n",
    "# cluster labels\n",
    "cluster_labels = np.zeros([X.shape[0]], dtype=int)\n",
    "for i in range(1,k):\n",
    "    cluster_labels[clusters[i]] = i\n",
    "\n",
    "# Step 4.ii.\n",
    "# Bin response variable to roughly so each group has roughly the same number of observations.\n",
    "response_binned = pd.qcut(y, q = k, labels = range(k))\n",
    "\n",
    "# Step4.iii.\n",
    "# NMI between clustering results and the binned categories.\n",
    "NMI_clusters3 = normalized_mutual_info_score(response_binned, cluster_labels)\n",
    "print(\"NMI score with clusters/K = 3:\", NMI_clusters3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial centers:  [ 109 1313]\n",
      "New Medoids are: [729, 1242]\n",
      "NMI score with clusters/K = 2: 0.39421252800175177\n"
     ]
    }
   ],
   "source": [
    "# Step 3.\n",
    "\n",
    "# Randomly sample 4 observations as initial centers.\n",
    "np.random.seed(50)\n",
    "k = 2\n",
    "center_index = np.random.randint(0, len(y),k)\n",
    "print(\"Initial centers: \",center_index)\n",
    "\n",
    "# create K-Medoids algorithm for processing distance matrix instead of points\n",
    "kmedoids_instance = kmedoids(gd_matrix, center_index, data_type='distance_matrix')\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedoids_instance.process()\n",
    "\n",
    "clusters = kmedoids_instance.get_clusters() \n",
    "medoids = kmedoids_instance.get_medoids() # new medoids\n",
    "print(\"New Medoids are:\",medoids)\n",
    "\n",
    "\n",
    "# Step 4.\n",
    "\n",
    "# Step 4.i.\n",
    "# cluster labels\n",
    "cluster_labels = np.zeros([X.shape[0]], dtype=int)\n",
    "for i in range(1,k):\n",
    "    cluster_labels[clusters[i]] = i\n",
    "\n",
    "# Step 4.ii.\n",
    "# Bin response variable to roughly so each group has roughly the same number of observations.\n",
    "response_binned = pd.qcut(y, q = k, labels = range(k))\n",
    "\n",
    "# Step4.iii.\n",
    "# NMI between clustering results and the binned categories.\n",
    "NMI_clusters2 = normalized_mutual_info_score(response_binned, cluster_labels)\n",
    "print(\"NMI score with clusters/K = 2:\", NMI_clusters2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "K = 2 clusters has highest NMI of 0.394 out of all and k = 5 has the lowest NMI of 0.249.\n",
    "As the number of K reduces, the NMI increases.\n",
    "\n",
    "NMI is a good measure for determining the quality of clustering.  \n",
    "It is an external measure because we need the class labels of the instances to determine the NMI.  \n",
    "Additionally, if the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient, Calinski-Harabasz index are examples of such an evaluation, where a higher score relates to a model with better defined clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html  \n",
    "    https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html  \n",
    "    https://course.ccs.neu.edu/cs6140sp15/7_locality_cluster/Assignment-6/NMI.pdf  \n",
    "    https://scikit-learn.org/stable/modules/clustering.html  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
